{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rsutils.modify_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_catalog_df = pd.read_csv('/gpfs/data1/cmongp2/sasirajann/nh_crop_calendar/crop_calendar/data/CHIRPS-v3.0/catalog.csv').sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_catalog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_gdf = gpd.read_file('/gpfs/data1/cmongp1/sasirajann/togo/shapefiles/Shapefiles/tgo_admbnda_adm0_inseed_itos_20210107.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_meta_list = \\\n",
    "# rsutils.modify_images.load_images(\n",
    "#     src_filepaths = chirps_catalog_df['local_filepath'],\n",
    "#     shapes_gdf = region_gdf,\n",
    "#     njobs = 120,\n",
    "# )\n",
    "\n",
    "# datacube = np.concatenate([data_meta[0] for data_meta in data_meta_list])\n",
    "# meta = data_meta_list[0][1]\n",
    "# valid_time = chirps_catalog_df['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d')).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\n",
    "#     '/gpfs/data1/cmongp2/sasirajann/nh_crop_calendar/crop_calendar/data/togo_datacube/datacube.npy',\n",
    "#     np.concatenate([data_meta[0] for data_meta in data_meta_list])\n",
    "# )\n",
    "\n",
    "# np.save(\n",
    "#     '/gpfs/data1/cmongp2/sasirajann/nh_crop_calendar/crop_calendar/data/togo_datacube/metadata.pickle.npy',\n",
    "#     {\n",
    "#         'geotiff_metadata': data_meta_list[0][1],\n",
    "#         'valid_time': chirps_catalog_df['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d')).to_numpy()\n",
    "#     },\n",
    "#     allow_pickle = True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube = np.load('/gpfs/data1/cmongp2/sasirajann/nh_crop_calendar/crop_calendar/data/togo_datacube/datacube.npy')\n",
    "metadata = np.load('/gpfs/data1/cmongp2/sasirajann/nh_crop_calendar/crop_calendar/data/togo_datacube/metadata.pickle.npy', allow_pickle=True)[()]\n",
    "\n",
    "valid_time = metadata['valid_time']\n",
    "meta = metadata['geotiff_metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_v3_togo_da = xr.DataArray(\n",
    "    datacube,\n",
    "    coords = {\n",
    "        'valid_time': valid_time,\n",
    "    },\n",
    "    dims = ('valid_time', 'x', 'y'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps_v3_togo_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10\n",
    "\n",
    "rev = chirps_v3_togo_da.sel(valid_time=chirps_v3_togo_da.valid_time[::-1])\n",
    "\n",
    "# rolling on reversed series (NaNs will be on the reversed \"start\", i.e. original end)\n",
    "rolling_rev = rev.rolling(valid_time=window, min_periods=window).sum()\n",
    "\n",
    "# reverse back to original time order\n",
    "rolling = rolling_rev.sel(valid_time=rolling_rev.valid_time[::-1])\n",
    "\n",
    "# rolling_doy_mean = rolling.groupby('valid_time.dayofyear').mean('valid_time').sel(dayofyear=slice(1, 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1984\n",
    "end_year = 2024\n",
    "\n",
    "rolling_mean_1984_2024 = rolling.where(\n",
    "    (rolling.valid_time.dt.year >= start_year) &\n",
    "    (rolling.valid_time.dt.year < end_year) &\n",
    "    (rolling.valid_time.dt.dayofyear >= 1) &\n",
    "    (rolling.valid_time.dt.dayofyear <= 365),\n",
    "    drop = True,\n",
    ").groupby('valid_time.dayofyear').mean('valid_time').sel(dayofyear=slice(1, 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1984\n",
    "end_year = 2004\n",
    "\n",
    "rolling_mean_1984_2004 = rolling.where(\n",
    "    (rolling.valid_time.dt.year >= start_year) &\n",
    "    (rolling.valid_time.dt.year < end_year) &\n",
    "    (rolling.valid_time.dt.dayofyear >= 1) &\n",
    "    (rolling.valid_time.dt.dayofyear <= 365),\n",
    "    drop = True,\n",
    ").groupby('valid_time.dayofyear').mean('valid_time').sel(dayofyear=slice(1, 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2004\n",
    "end_year = 2024\n",
    "\n",
    "rolling_mean_2004_2024 = rolling.where(\n",
    "    (rolling.valid_time.dt.year >= start_year) &\n",
    "    (rolling.valid_time.dt.year < end_year) &\n",
    "    (rolling.valid_time.dt.dayofyear >= 1) &\n",
    "    (rolling.valid_time.dt.dayofyear <= 365),\n",
    "    drop = True,\n",
    ").groupby('valid_time.dayofyear').mean('valid_time').sel(dayofyear=slice(1, 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "\n",
    "rolling.where(\n",
    "    rolling.valid_time.dt.year.isin(year) &\n",
    "    (rolling.valid_time.dt.dayofyear >= 1) &\n",
    "    (rolling.valid_time.dt.dayofyear <= 365),\n",
    "    drop = True,\n",
    ").sel(x = 50, y=20).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean_1984_2004.sel(x = 50, y=20).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean_2004_2024.sel(x = 50, y=20).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import rsutils.utils\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_clusters_by_count(cluster_ids:np.ndarray):\n",
    "    _ids, _counts = np.unique(cluster_ids, return_counts=True)\n",
    "    cluster_count_df = pd.DataFrame(data={\n",
    "        'cluster_id': _ids,\n",
    "        'count': _counts\n",
    "    })\n",
    "    cluster_count_df = cluster_count_df.sort_values(by='count', ascending=False)\n",
    "    cluster_count_df['new_cluster_id'] = range(_ids.shape[0])\n",
    "    new_cluster_id_map = dict(zip(\n",
    "        cluster_count_df['cluster_id'],\n",
    "        cluster_count_df['new_cluster_id'],\n",
    "    ))\n",
    "    new_cluster_ids = np.zeros(shape=cluster_ids.shape)\n",
    "    for old_id, new_id in new_cluster_id_map.items():\n",
    "        new_cluster_ids[cluster_ids == old_id] = new_id\n",
    "    return new_cluster_ids.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_folderpath = '/gpfs/data1/cmongp2/sasirajann/nh_crop_calendar/crop_calendar/data/togo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_clustered_lineplots(...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_clustered_lineplots(\n",
    "    crop_name:str,\n",
    "    band_name:str,\n",
    "    timeseries:np.ndarray,\n",
    "    x:list,\n",
    "    cluster_ids:np.ndarray, \n",
    "    save_filepath:str,\n",
    "    alpha:float=0.05,\n",
    "    y_min:float=-0.3,\n",
    "    y_max:float=0.9,\n",
    "    scale:float=5,\n",
    "    aspect_ratio:float=2,\n",
    "    nrows:int=3,\n",
    "    ncols:int=3,\n",
    "    limit_plots_per_cluster:int=1000,\n",
    "    random_state:int=42,\n",
    "    x_label:str='dates',\n",
    "    cluster_id_to_color_map:dict=None,\n",
    "    x_label_rotation:float = 0,\n",
    "    ignore_clusters = [],\n",
    "):\n",
    "    n_points, n_timestamps = timeseries.shape\n",
    "\n",
    "    if len(x) != n_timestamps:\n",
    "        raise ValueError('Length of y should match timeseries shape.')\n",
    "    \n",
    "    unique_cluster_ids, counts = np.unique(cluster_ids, return_counts=True)\n",
    "    cluster_counts_df = pd.DataFrame(data={'cluster_id':unique_cluster_ids,'count':counts})\n",
    "    sorted_cluster_ids = cluster_counts_df.sort_values(\n",
    "        by=['count', 'cluster_id'], ascending=[False, True],\n",
    "    )['cluster_id']\n",
    "\n",
    "    n_clusters = len(unique_cluster_ids)\n",
    "    if n_clusters - len(set(ignore_clusters)) > nrows * ncols:\n",
    "        raise ValueError(\n",
    "            f'Too many clusters for too less nrows and ncols. '\n",
    "            f'n_clusters = {n_clusters} > nrows * ncols = {nrows * ncols}'\n",
    "        )\n",
    "\n",
    "    _df = pd.DataFrame(\n",
    "        data=np.concatenate([\n",
    "            np.array([range(n_points)]).T, \n",
    "            timeseries, \n",
    "            np.array([cluster_ids]).T\n",
    "        ], axis=1),\n",
    "        columns=['id'] + list(x) + ['cluster_id']\n",
    "    )\n",
    "    _df['id'] = _df['id'].astype(int)\n",
    "    _df['cluster_id'] = _df['cluster_id'].astype(int)\n",
    "    melted_dfs = []\n",
    "    for cluster_id in sorted_cluster_ids:\n",
    "        df_to_melt_i = _df[_df['cluster_id']==cluster_id]\n",
    "        melted_df_i = df_to_melt_i[_df.columns[:-1]].melt(\n",
    "            id_vars='id',\n",
    "            var_name=x_label, \n",
    "            value_name=band_name,\n",
    "        )\n",
    "        melted_df_i['cluster_id'] = cluster_id\n",
    "        melted_dfs.append(melted_df_i)\n",
    "    _df_melted = pd.concat(melted_dfs)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(scale*aspect_ratio*ncols, scale*nrows))\n",
    "    i, j = 0, 0\n",
    "\n",
    "    for index, cluster_number in enumerate(sorted_cluster_ids):\n",
    "        if cluster_number in ignore_clusters:\n",
    "            continue\n",
    "        \n",
    "        if ncols > 1 and nrows > 1:\n",
    "            ax = axs[j][i]\n",
    "            if i == ncols:\n",
    "                j += 1\n",
    "                i = 0\n",
    "        else:\n",
    "            ax = axs[i]\n",
    "        \n",
    "        i += 1\n",
    "            \n",
    "        _df_plot = _df_melted[_df_melted['cluster_id']==cluster_number]\n",
    "        count = _df_plot['id'].unique().shape[0]\n",
    "        ids = _df_plot['id'].unique().tolist()\n",
    "        np.random.RandomState(seed=random_state).shuffle(ids)\n",
    "        selected_ids = ids[:limit_plots_per_cluster]\n",
    "        _df_plot = _df_plot[_df_plot['id'].isin(selected_ids)]\n",
    "        limited_count = _df_plot['id'].unique().shape[0]\n",
    "\n",
    "        if cluster_id_to_color_map is None:\n",
    "            color = sns.palettes.color_palette()[index % 10]\n",
    "        else:\n",
    "            color = cluster_id_to_color_map[cluster_number]\n",
    "\n",
    "        g = sns.lineplot(\n",
    "            data=_df_plot,\n",
    "            ax=ax,\n",
    "            x=x_label,\n",
    "            y=band_name,\n",
    "            hue='id',\n",
    "            alpha=alpha,\n",
    "            legend=False,\n",
    "            palette=[\n",
    "                color for _ in range(limited_count)\n",
    "            ],\n",
    "        )\n",
    "        g.set_ylim([y_min, y_max])\n",
    "        g.set_title(f'crop: {crop_name}, cluster_id: {cluster_number}, count: {count}')\n",
    "        g.grid()\n",
    "\n",
    "        if x_label_rotation != 0:\n",
    "            g.set_xticklabels(g.get_xticklabels(), rotation=x_label_rotation)\n",
    "\n",
    "    if nrows > 1 and ncols > 1:\n",
    "        while j < nrows:\n",
    "            fig.delaxes(axs[j][i])\n",
    "            i += 1\n",
    "            if i == ncols:\n",
    "                j += 1\n",
    "                i = 0\n",
    "\n",
    "    fig.savefig(save_filepath, bbox_inches='tight')\n",
    "\n",
    "    plt.close()\n",
    "    plt.cla()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'rolling{window}_1984-2024_doy_mean'\n",
    "band_name = f'rolling sum ({window} days) rainfall DOY mean'\n",
    "data = rolling_mean_1984_2024\n",
    "\n",
    "n_ts, height, width = data.values.shape\n",
    "\n",
    "data_2d = data.values.reshape(n_ts, height*width).swapaxes(0, 1)\n",
    "data_2d.shape\n",
    "\n",
    "y_max = np.ceil(data.max().data)\n",
    "\n",
    "n_clusters = 3\n",
    "nrows, ncols = 2, 1\n",
    "\n",
    "cluster_ids = sklearn.cluster.MiniBatchKMeans(\n",
    "    n_clusters = n_clusters,\n",
    "    random_state = 42,\n",
    ").fit(data_2d).labels_\n",
    "\n",
    "cluster_ids = relabel_clusters_by_count(cluster_ids=cluster_ids)\n",
    "\n",
    "plot_clustered_lineplots(\n",
    "    crop_name = '',\n",
    "    band_name = band_name,\n",
    "    timeseries = data_2d,\n",
    "    cluster_ids = cluster_ids,\n",
    "    y_min = -1,\n",
    "    y_max = y_max,\n",
    "    nrows = nrows,\n",
    "    ncols = ncols,\n",
    "    x = range(1, n_ts + 1),\n",
    "    x_label = 'DOY',\n",
    "    save_filepath = os.path.join(export_folderpath, f'{filename}.png'),\n",
    "    alpha = 0.02,\n",
    "    aspect_ratio = 1.5,\n",
    "    ignore_clusters = [0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids_2d = cluster_ids.reshape(height, width).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.update({\n",
    "    'dtype': 'uint8',\n",
    "    'nodata': 0,\n",
    "})\n",
    "\n",
    "with rasterio.open(os.path.join(export_folderpath, f'{filename}.tif'), 'w', **meta) as dst:\n",
    "    dst.write(cluster_ids_2d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bimodality tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_bimodal = rolling_mean_1984_2004.sel(x = 20, y=20).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimodal = rolling_mean_2004_2024.sel(x = 90, y=20).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shouldnt_by_bimodal = rolling_mean_1984_2004.sel(x = 80, y=20).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prominence_ratio(timeseries,  prominence=10):\n",
    "    peaks, props = scipy.signal.find_peaks(timeseries, prominence=prominence)\n",
    "    n_peaks = len(peaks)\n",
    "\n",
    "    if n_peaks >= 2:\n",
    "        prom = props[\"prominences\"]\n",
    "        prom_ratio = prom[np.argsort(not_bimodal[peaks])][-2:]\n",
    "        prominence_ratio = min(prom_ratio) / max(prom_ratio)\n",
    "    else:\n",
    "        prominence_ratio = 0\n",
    "    \n",
    "    return prominence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harmonic_ratio(timeseries):\n",
    "    fft = np.fft.rfft(timeseries - timeseries.mean())\n",
    "    amps = np.abs(fft)\n",
    "    A1 = amps[1]   # 1 cycle/yr\n",
    "    A2 = amps[2]   # 2 cycles/yr\n",
    "    BPI = A2 / (A1 + 1e-12)\n",
    "    return BPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 5\n",
    "aspect_ratio = 1.3\n",
    "fig, ax = plt.subplots(figsize=(scale*aspect_ratio, scale))\n",
    "\n",
    "for ts, label in [\n",
    "    (not_bimodal, 'not bimodal'),\n",
    "    (bimodal, 'bimodal'),\n",
    "    # (shouldnt_by_bimodal, 'should not be bimodal'),\n",
    "]:\n",
    "    pr = get_prominence_ratio(ts)\n",
    "    hr = get_harmonic_ratio(ts)\n",
    "    ax.plot(ts, label=f'{label} (PR = {round(pr, 2)}, HR = {round(hr, 2)})')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test randomly selected cluster, or plot distrbution on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mean_1984_2024.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rolling_mean_1984_2024\n",
    "\n",
    "_, nx, ny = data.shape\n",
    "\n",
    "pr_array = np.zeros(shape=(nx, ny))\n",
    "hr_array = np.zeros(shape=(nx, ny))\n",
    "\n",
    "for x in range(nx):\n",
    "    for y in range(ny):\n",
    "        pr = get_prominence_ratio(\n",
    "            timeseries = data.sel(x=x, y=y).data,\n",
    "            prominence = 10,\n",
    "        )\n",
    "        hr = get_harmonic_ratio(\n",
    "            timeseries = data.sel(x=x, y=y).data,\n",
    "        )\n",
    "        pr_array[x, y] = pr\n",
    "        hr_array[x, y] = hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pr_df = pd.DataFrame(\n",
    "    data = {\n",
    "        'cluster id': cluster_ids_2d.flatten(),\n",
    "        'prominence ratio': pr_array.flatten(),\n",
    "        'harmonic ratio': hr_array.flatten(),\n",
    "    }\n",
    ")\n",
    "\n",
    "cluster_pr_df = cluster_pr_df[cluster_pr_df['cluster id'] != 0]\n",
    "\n",
    "scale = 5\n",
    "aspect_ratio = 1\n",
    "ncols = 2\n",
    "NBINS = 50\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(scale*ncols, scale*aspect_ratio), ncols=ncols)\n",
    "\n",
    "g = sns.histplot(\n",
    "    ax = axs[0],\n",
    "    data = cluster_pr_df,\n",
    "    x = 'prominence ratio',\n",
    "    hue = 'cluster id',\n",
    "    bins = NBINS,\n",
    ")\n",
    "\n",
    "g = sns.histplot(\n",
    "    ax = axs[1],\n",
    "    data = cluster_pr_df,\n",
    "    x = 'harmonic ratio',\n",
    "    hue = 'cluster id',\n",
    "    bins = NBINS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 4\n",
    "aspect_ratio = 2\n",
    "ncols = 2\n",
    "fig, axs = plt.subplots(figsize=(scale*ncols, scale*aspect_ratio), ncols=ncols)\n",
    "\n",
    "g = sns.heatmap(data=pr_array, ax=axs[0])\n",
    "_ = g.set_title('Prominence Ratio Test')\n",
    "\n",
    "g = sns.heatmap(data=hr_array, ax=axs[1])\n",
    "_ = g.set_title('Harmonic Ratio Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = shouldnt_by_bimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nh_crop_calendar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
